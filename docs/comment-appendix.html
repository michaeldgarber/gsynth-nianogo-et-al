<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Michael D. Garber" />

<meta name="date" content="2024-01-31" />

<title>Appendix for comment on Nianogo et al.</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="https://michaeldgarber.github.io/">
    <span class="fa fa-home"></span>
     
    MDG
  </a>
</li>
<li>
  <a href="index.html">Comment on Nianogo et al.</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Appendix for comment on Nianogo et al.</h1>
<h4 class="author">Michael D. Garber</h4>
<h4 class="date">2024-01-31</h4>

</div>


<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This appendix contains supporting information and code related to my article, <a href="https://journals.lww.com/epidem/fulltext/2024/03000/precision_and_weighting_of_effects_estimated_by.17.aspx">Precision and Weighting of Effects Estimated by the Generalized Synthetic Control and Related Methods: The Case of Medicaid Expansion</a>, a comment on Nianogo et al.’s <a href="https://journals.lww.com/epidem/fulltext/2024/03000/medicaid_expansion_and_racial_ethnic_and_sex.16.aspx">article</a>.</p>
<p>The R Markdown file that creates this web page is located here:
<a href="https://github.com/michaeldgarber/gsynth-nianogo-et-al/blob/main/docs/comment-appendix.Rmd" class="uri">https://github.com/michaeldgarber/gsynth-nianogo-et-al/blob/main/docs/comment-appendix.Rmd</a></p>
<p>Nianogo and colleagues provide their data and code here:
<a href="https://github.com/nianogo/Medicaid-CVD-Disparities" class="uri">https://github.com/nianogo/Medicaid-CVD-Disparities</a></p>
<p>I’ve done some additional processing to that data in the scripts located in this folder:
<a href="https://github.com/michaeldgarber/gsynth-nianogo-et-al/tree/main/scripts" class="uri">https://github.com/michaeldgarber/gsynth-nianogo-et-al/tree/main/scripts</a></p>
<p>Those scripts can be run in the following order:</p>
<pre class="r"><code>source(here(&quot;scripts&quot;, &quot;read-wrangle-data-acs.R&quot;))
source(here(&quot;scripts&quot;, &quot;read-wrangle-data-pres-election.R&quot;))
source(here(&quot;scripts&quot;, &quot;read-wrangle-data-nianogo-et-al.R&quot;))</code></pre>
<p>Packages used</p>
<pre class="r"><code>library(here)
library(tidyverse)
library(mapview) #for interactive mapping
library(tmap) #static mapping
library(sf) #managing spatial data
library(gsynth) #to run the generalized synthetic control method
library(RColorBrewer) #for creating color palettes
library(viridis) #more color palettes</code></pre>
</div>
<div id="exploring-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Exploring data</h1>
<div id="medicaid-expansion-and-missingness" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Medicaid expansion and missingness</h2>
<p>In this section, I explore Medicaid expansion status by missingness status in the demographic groups.</p>
<div id="medicaid-expansion-all-states-2019" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Medicaid expansion (all states, 2019)</h3>
<p>This map shows Medicaid expansion status of states in 2019.</p>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 2.1: </span>Medicaid expansion status (2019)</caption>
<thead>
<tr class="header">
<th align="right">treated</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">16</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">34</td>
</tr>
</tbody>
</table>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="hispanic-population" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Hispanic population</h3>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 2.2: </span>Medicaid expansion status (2019) among states with non-missing CVD data for the Hispanic population</caption>
<thead>
<tr class="header">
<th align="right">treated</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">20</td>
</tr>
</tbody>
</table>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="black-population" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Black population</h3>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 2.3: </span>Medicaid expansion status (2019) among states with non-missing CVD data for the Black population</caption>
<thead>
<tr class="header">
<th align="right">treated</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">25</td>
</tr>
</tbody>
</table>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="distribution-of-brfss-covariates" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Distribution of BRFSS covariates</h2>
<p>In this section, I examine the distribution of some of the BRFSS covariates used in the analysis to see if the BRFSS data may be contributing to the instability of the effect estimates.</p>
<div id="proportion-men" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Proportion men</h3>
<div id="all-adults-aged-45-64" class="section level4" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> All adults aged 45-64</h4>
<p>Proportion men among all adults aged 45-64 (dataset name: overall)</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.3093  0.3957  0.4166  0.4173  0.4400  0.5317</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="hispanic-adults-aged-45-64" class="section level4" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> Hispanic adults aged 45-64</h4>
<p>Proportion men among Hispanic adults aged 45-64 (dataset name: hispanic_complete)</p>
<p>Observation: some state-years have 0% or 100% men in this group, which is not plausible, highlighting the fact that BRFSS may not be reliable in such stratified sub-groups.</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.3693  0.4200  0.4188  0.4651  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>In California, most years seem implausibly low.
<img src="comment-appendix_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="black-adults-aged-45-64" class="section level4" number="2.2.1.3">
<h4><span class="header-section-number">2.2.1.3</span> Black adults aged 45-64</h4>
<p>Again, the proportion men among Black adults aged 45-64 seems implausible in some state-years.</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1579  0.3250  0.3643  0.3761  0.4194  0.7000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="white-adults-aged-45-64" class="section level4" number="2.2.1.4">
<h4><span class="header-section-number">2.2.1.4</span> White adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.3598  0.4196  0.4198  0.4742  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="low-income" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Low income</h3>
<p>Less than $15,000</p>
<div id="all-adults-aged-45-64-1" class="section level4" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> All adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.03898 0.06782 0.08677 0.09121 0.11007 0.19325</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="hispanic-adults-aged-45-64-1" class="section level4" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Hispanic adults aged 45-64</h4>
<p>Again, there are what I would think are some implausible observations (e.g,. 0% or 100% low income in a state-year).</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.1143  0.1533  0.1586  0.1968  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="black-adults-aged-45-64-1" class="section level4" number="2.2.2.3">
<h4><span class="header-section-number">2.2.2.3</span> Black adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.1355  0.1765  0.1784  0.2166  0.5000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="white-adults-aged-45-64-1" class="section level4" number="2.2.2.4">
<h4><span class="header-section-number">2.2.2.4</span> White adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.00000 0.09858 0.14219 0.14602 0.19149 1.00000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="low-education" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Low education</h3>
<p>No high-school degree</p>
<div id="all-adults-aged-45-64-2" class="section level4" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> All adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.01872 0.04614 0.06653 0.07496 0.09833 0.23663</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="hispanic-adults-aged-45-64-2" class="section level4" number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Hispanic adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.1780  0.2650  0.2611  0.3486  0.5547</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="black-adults-aged-45-64-2" class="section level4" number="2.2.3.3">
<h4><span class="header-section-number">2.2.3.3</span> Black adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.00000 0.08062 0.11265 0.11936 0.15205 0.50000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="white-adults-aged-45-64-2" class="section level4" number="2.2.3.4">
<h4><span class="header-section-number">2.2.3.4</span> White adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.1250  0.2268  0.2318  0.3333  0.6400</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>
<div id="married" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Married</h3>
<div id="all-adults-aged-45-64-3" class="section level4" number="2.2.4.1">
<h4><span class="header-section-number">2.2.4.1</span> All adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.5029  0.5849  0.6104  0.6137  0.6385  0.7633</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="hispanic-adults-aged-45-64-3" class="section level4" number="2.2.4.2">
<h4><span class="header-section-number">2.2.4.2</span> Hispanic adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.5245  0.5869  0.5759  0.6317  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<div id="black-adults-aged-45-64-3" class="section level4" number="2.2.4.3">
<h4><span class="header-section-number">2.2.4.3</span> Black adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.3279  0.3735  0.3708  0.4106  0.6111</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="white-adults-aged-45-64-3" class="section level4" number="2.2.4.4">
<h4><span class="header-section-number">2.2.4.4</span> White adults aged 45-64</h4>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.5226  0.5890  0.5810  0.6429  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
</div>
<div id="political-orientation-and-considering-an-alternative-measure" class="section level3" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Political orientation and considering an alternative measure</h3>
<p>In Nianogo et al.’s Table 1, they present the measure of political-party affiliation in 2014.
The below replicates those values, where 0=Republican, 1=Democrat, and 2=Split.</p>
<table>
<thead>
<tr class="header">
<th align="right">party</th>
<th align="right">n</th>
<th align="right">n_total</th>
<th align="right">prop</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">24</td>
<td align="right">50</td>
<td align="right">0.48</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">13</td>
<td align="right">50</td>
<td align="right">0.26</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">13</td>
<td align="right">50</td>
<td align="right">0.26</td>
</tr>
</tbody>
</table>
<p>To facilitate interpretation, this variable in 2016 is mapped here:
<img src="comment-appendix_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>These values struck me as somewhat odd. For one, it appears a given state-year receives a value of either Republican, Democrat, or split, raising the question of within-state variability in the measure. Second, given the polarization and parity of party politics in the United States, I would expect the values corresponding to Republican (0) and Democrat (1) to be about the same and for both to be nearer to 50%.</p>
<p>A simpler and more stable (in terms of sampling variability) measure for the state’s political environment might be the popular-vote share from presidential elections. Those data are available here:</p>
<p><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/42MVDX" class="uri">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/42MVDX</a></p>
<p>In this script, I’ve loaded popular-vote data from presidential elections.</p>
<pre class="r"><code>source(here(&quot;scripts&quot;, &quot;read-wrangle-data-pres-election.R&quot;))</code></pre>
<p>In analyses as presented in Scenarios 2-5 in the main text, I applied the vote share from the most recent election if the year wasn’t an election year. For example, 2014 received 2012’s popular-vote data.</p>
</div>
</div>
</div>
<div id="using-state-year-effect-estimates-to-calculate-various-summary-measures" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Using state-year effect estimates to calculate various summary measures</h1>
<p>In this section, I show how state-year effect estimates generated by the <code>gsynth()</code> function can be used to calculate summary measures of effect. Specifically, in this section, I populate some values corresponding to Scenarios 4 and 5 in the main text’s table.</p>
<p>Code that generates other results can be found here:
gsynth-nianogo-et-al/scripts/gsynth-analyses-to-post.R</p>
<div id="load-the-data" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Load the data</h2>
<pre class="r"><code>source(here(&quot;scripts&quot;, &quot;read-wrangle-data-acs.R&quot;))
source(here(&quot;scripts&quot;, &quot;read-wrangle-data-pres-election.R&quot;))
source(here(&quot;scripts&quot;, &quot;read-wrangle-data-nianogo-et-al.R&quot;))</code></pre>
</div>
<div id="run-the-gsynth-function" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Run the gsynth function</h2>
<p>Here, I will summarize effects estimated by the MC-NNM estimator as presented in Scenarios 4 and 5 of the table in the text.</p>
<p>For more information on the syntax of the <code>gsynth()</code> function, please see:
<a href="https://yiqingxu.org/packages/gsynth/articles/tutorial.html#matrix-completion" class="uri">https://yiqingxu.org/packages/gsynth/articles/tutorial.html#matrix-completion</a></p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm=gsynth(
  #syntax: outcome ~ treatment indicator+ covariates
  cvd_death_rate ~ treatedpost + 
    primarycare_rate +
    cardio_rate + 
    population_overall + 
    low_educ_overall + 
    married_overall+ 
    employed_for_wages_overall + 
    vote_share_dem+#popular vote data
    low_income_overall + 
    male_overall + 
    race_nonwhite_overall
  ,

  #dataset in which the effects are estimated
  data = overall_covars_alt,
  estimator = &quot;mc&quot;, #the MC-NNM estimator
  #      EM = F, #
  index = c(&quot;state_id&quot;,&quot;year&quot;), #time-unit
  
  #non-parametric bootstrap if MC-NNM estimator used
  #inference = &quot;parametric&quot;, 
  se = TRUE, 
  
  #perform a cross-validation procedure to 
  #determine the number of unobserved factors
  CV = TRUE, 

  #the range of possible numbers of unobserved factors
  r = c(0, 5), #
  seed = 123, #arbitrary seed so same results every time
  nboots = 2000, #number of bootstrap reps
  force = &quot;two-way&quot;, 
  parallel = TRUE
)</code></pre>
</div>
<div id="examine-summary-output-returned-by-the-gsynth-function" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Examine summary output returned by the gsynth function</h2>
<div id="average-treatment-effect-with-confidence-intervals" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Average treatment effect with confidence intervals</h3>
<p>Return the average (unweighted) difference effect over all treated state-years and the corresponding standard error and confidence intervals.</p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm$est.avg</code></pre>
<pre><code>##          Estimate     S.E.  CI.lower CI.upper   p.value
## ATT.avg -2.118079 2.163304 -6.358078 2.121919 0.3275332</code></pre>
<p>These confidence intervals are calculated by adding 1.96<em>SE and -1.96</em>SE to either side of the estimate.</p>
<pre class="r"><code>#qnorm(.975)#return the exact value
#Estimate
gsynth_out_overall_sub_pol_mc_nnm$est.avg[1]</code></pre>
<pre><code>## [1] -2.118079</code></pre>
<pre class="r"><code>#Upper limit
gsynth_out_overall_sub_pol_mc_nnm$est.avg[1]+
  gsynth_out_overall_sub_pol_mc_nnm$est.avg[2]*1.959964</code></pre>
<pre><code>## [1] 2.121919</code></pre>
<pre class="r"><code>#Lower limit
gsynth_out_overall_sub_pol_mc_nnm$est.avg[1]-
  gsynth_out_overall_sub_pol_mc_nnm$est.avg[2]*1.959964</code></pre>
<pre><code>## [1] -6.358078</code></pre>
</div>
<div id="average-treatment-effect---point-estimate-only" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Average treatment effect - point estimate only</h3>
<p>Another way to return the point estimate for the average treatment effect (without the confidence intervals) is by <code>gsynth_object$att.avg</code>.</p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm$att.avg</code></pre>
<pre><code>## [1] -2.118079</code></pre>
</div>
<div id="average-treatment-effect-at-each-time-point" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Average treatment effect at each time point</h3>
<p>The average treatment effect at each time point can be returned by <code>gsynth_object$att</code>. These effect estimates are summarized by time over units where time is indexed from the treatment period for that unit such that the first treated time point is time 1. By the numbering system below, time 0 is the latest time point of the pre-treatment period.</p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm$att</code></pre>
<pre><code>##         -13         -12         -11         -10          -9          -8 
## -0.26992263  0.38732901  0.26348340 -0.79655792 -1.01552488  0.23192784 
##          -7          -6          -5          -4          -3          -2 
##  0.08756312  0.14899219  0.23311709 -0.07675195 -0.03369085  0.30835163 
##          -1           0           1           2           3           4 
##  0.40214995  0.14395025 -3.94572630 -4.90217808 -0.48894728 -1.83082518 
##           5           6 
## -1.53387942  0.56269021</code></pre>
</div>
</div>
<div id="calculate-unweighted-average-difference-effect-by-summarizing-effect-estimates-over-treated-state-years" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Calculate unweighted average difference effect by summarizing effect estimates over treated state-years</h2>
<div id="return-effect-estimates-for-every-treated-state-year" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Return effect estimates for every treated state-year</h3>
<p>Using the effect estimates for every treated state-year, we can replicate the summary difference effects presented above. Effect estimates for every treated state-year are returned by <code>gsynth_objecct$eff</code>.</p>
<p>Here are the difference effects for all state-years. Note that “effects” are estimated for all state-years in the treated states, including pre-treatment years. Pre-treatment effects are not really effects but are the pre-treatment prediction error for treated units: the difference between the observed and predicted counterfactual outcomes before treatment.</p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm$eff</code></pre>
<pre><code>##                2           4           5           6           8            9
## 2000  -2.8549609   0.9254919 -6.56021258  -6.3914770  -2.9607249   5.01758412
## 2001   5.8785560  -2.8025061 -3.46542138  -9.7262748 -10.6700147   1.56284433
## 2002  -0.3513856   3.3130764  5.35241334  -4.2376838   3.4085189   3.31272662
## 2003  -0.3155085  -3.1909703 -9.84283260   2.9835159  -1.1003219  -1.01984959
## 2004   0.5178832   1.2243677 -2.37844455   0.3360901   0.1558620  -0.15402197
## 2005  -5.3910471   2.8557760  0.05381347   0.1484376  -0.5027154  -2.92327032
## 2006  -9.3061276   3.6482257  5.60159505   6.3278789   0.8159762  -4.80527797
## 2007 -13.9966886   2.6698943 -0.12056547   5.3221665   1.3100673  -1.99285818
## 2008  -8.0616089  -3.8467589  5.45840980  -0.2389340   1.7882295  -0.05959601
## 2009   2.2731099  -3.1360315 -4.39773648   1.2113080   5.3525481   0.83235292
## 2010   9.6699464  -4.9302006  1.99157065   1.4377013  -0.4254949   0.93916143
## 2011   4.1806033   1.5230066  6.41778331   0.6398288   3.8678738  -1.52598728
## 2012   6.6324375   2.0246665  5.80748351   4.5018306   3.5084971   5.70294337
## 2013   1.1994690  -0.4073624 -2.26575154  -1.2320234  -2.7471547  -4.53157237
## 2014  12.4764326 -10.2414970 16.40508485  -8.8986156  -0.3509733 -13.46949116
## 2015  11.7806796  -5.9121541  4.40407449 -10.7073366  -2.7536341 -15.30211467
## 2016  10.6629440   2.5407575 14.36990400  -0.6185028   5.6350307 -15.82618933
## 2017   2.7299284  -4.5502136 20.27686278  -3.7604160  -3.9764594 -14.49889136
## 2018  11.5267968  -5.8068421 13.01575069  -4.0113799  -8.7816937  -9.97251318
## 2019  14.0531558  -5.9749003 28.88296298  -4.6492172  -0.4304725 -11.75555220
##                10          15          17         18          19           20
## 2000  -1.81602305   8.0833229   1.9257576  2.3235016   0.1176644 -5.555199395
## 2001   2.13907177  -5.0601838   3.6297837 -2.6650165  -4.8037043  2.217920042
## 2002  -4.35112520   4.3777023   2.1085935 -3.9036759  -7.6005317 -0.915802086
## 2003  -7.92495992  -3.9467320   0.1489462  0.6781281   2.6342882 -0.006407251
## 2004   2.01941293  -0.6863185   0.2391470  4.0858052  -3.3363742  5.469241138
## 2005  -0.45835661   1.6764130   0.6290714 -2.2168794  -1.1991227  3.413243076
## 2006   4.73311789  -0.9313507  -4.2223637 -0.4899900   2.9689752 -0.978106611
## 2007   4.08137468  -9.6010019  -1.3708982 -1.2385030  -1.1254830 -3.807699360
## 2008   8.67188119   2.5259067   3.4691970  0.5785341  -2.2654531 -1.413289722
## 2009   3.44429572   1.4708106  -0.4549505 -2.8616421   1.2561041 -2.850611758
## 2010   0.07237734  -4.9620191  -0.1246502  2.3606650   5.7387897  0.746911783
## 2011 -12.91829889   1.4193114  -0.1147696  1.4696709   1.1421687  1.697238737
## 2012  -6.88068309   1.8411968  -4.1509112  4.8162724   2.7783652  4.608116170
## 2013   7.89468907   4.2960914  -2.5491895  0.7139686   4.6177348 -1.502840268
## 2014 -22.89346887  19.0004806 -11.6410006 -3.3863784 -10.4195965  5.991426220
## 2015 -22.89838332   9.4663920 -14.7722243 -8.2851715 -15.3251365  7.786098766
## 2016 -16.42311988 -12.5374204 -11.3011604  2.3463428   1.2881646  9.341081057
## 2017 -11.29206388   8.4828745 -16.6572568 -2.0741844  -0.6877451  5.604985932
## 2018 -21.13442854   4.5800173  -9.1374801  0.3032116   0.9905305  9.965214467
## 2019 -23.44436702   5.6359251 -13.4046862  1.3658028  19.8335289  6.429658030
##              21          22           23          24          25          26
## 2000 -4.6294111  -1.7437083   0.07570525   2.7204380  -0.6990843  -0.0161745
## 2001  6.4119219   2.6067579   6.69923992   0.6281304  -0.4088048  -3.3504597
## 2002 -5.0860523  -6.1868444  -6.02419187  -2.9893821   2.9089778  -2.5980492
## 2003  0.5004803   3.6138866  -1.15556061  -1.6482439   6.5289678   1.5007905
## 2004 -1.9404038  -1.1781295  -5.84272521   4.6644422   4.4017002  -2.8865029
## 2005  0.5737142   2.4002428   3.15175303   0.2256716  -3.3073082   4.1748908
## 2006 -0.2055802   2.2460686   9.59173002   0.1218915  -2.1117501  -2.9027444
## 2007 -4.0766510   0.9936110   3.68216281   1.3875399   2.9845796   3.3115265
## 2008 -3.2261717   8.1203126  -0.75057774  -3.0707603  -1.3134053   0.6602714
## 2009 -0.8405247  -0.0926782 -10.05108079   4.9282587   0.4090805  -1.0117059
## 2010  1.3752575  -3.4818804  -5.42439483   2.9718065  -2.5460516  -0.6252006
## 2011  2.3199656 -12.9166318   5.97492694  -4.9875114  -1.4695736   5.0311348
## 2012  8.8105395   0.5373354  -0.36686424  -3.5569524  -1.1951496  -0.6589605
## 2013  1.6694429   1.7842381  -3.20559413  -2.3161417  -4.0163692  -0.8177121
## 2014  6.8212288   6.7992718  -1.26734251 -12.1052757 -10.2694142  -3.7923500
## 2015 12.7207553  -3.9471131   8.21634494  -7.1830097 -12.1394040  -9.1702556
## 2016 23.5383831   3.6295946   1.36876148  -5.0372080 -10.0868757   3.8551289
## 2017 15.5653180   6.7371183  -1.84861304 -10.3177240 -11.1426868  -6.6989052
## 2018 16.1854734  -4.3213319  -3.00867593  -5.0233898 -12.5516589  -7.1359537
## 2019 19.4853982  -2.7256552  -6.19037815  -8.6562222 -10.0813651 -13.5660754
##              27          30           32         33           34         35
## 2000 -4.0048696  -1.3426304  -4.65485271   1.297806   3.49322602 -8.7826231
## 2001 -0.1704084  -1.0810075   2.14520702   4.990289   1.89685211 -1.1569962
## 2002 -2.0120038   4.5521697  -4.44486600   6.015901   1.83996172  3.6758075
## 2003 -4.9673122   6.4480646  -1.11728952 -10.111013   0.70623219 -5.3882315
## 2004  0.1339679 -10.5633307   1.44698982  -5.876708  -1.34815685 -6.9289465
## 2005  2.9537924  -4.9570906  14.73236650  -4.786283   2.87606329  2.2905988
## 2006 -1.6440201  -6.6824458   7.34860789   6.833601  -0.05237921 -3.2057401
## 2007  1.0162709   7.9289953  -8.30998500   4.348816  -3.12345705  3.5031667
## 2008  0.4591471   1.6399859  -5.22629959  -4.437997  -1.47227800  0.7100342
## 2009  1.8882641   1.3506144   7.05149787   3.132310 -12.37021673  6.4353284
## 2010  0.1269997   1.1665361  -4.22738418  -6.564625   6.54540853  4.3688844
## 2011  1.7030661   2.4417498  -0.99729630  -1.901203   1.24024958  4.3837180
## 2012  4.4449333  -0.2856542  -4.29067883   1.298751   1.08499560 -5.0201027
## 2013  1.0497289   2.2230876  -0.07108479   5.590018  -1.33348534  6.6105782
## 2014 -4.1888925  -5.1417667  -8.98697513 -11.272948  -7.67655067  3.1359613
## 2015 -8.0823884   2.2432412  -2.12211379 -10.839888 -12.53180244 -5.7178546
## 2016 -2.6465417  -4.0264410   0.90474778   4.272769  -8.07086934 21.6165209
## 2017  0.9229848  -3.4163772 -12.80839412   1.648234  -7.25530872  7.0654661
## 2018 -0.5146687  14.1233195 -15.25640221  10.693277  -6.43962457  6.6653689
## 2019 -2.9910751   9.3273563  -7.47229206   6.713150  -9.11903698 23.2141307
##               36          38           39           41         42          44
## 2000   1.4980350  -0.7849287   2.72070183  -2.74410407  4.7260493  -8.7893744
## 2001   2.2277692   8.5343622   2.15891822   0.19279412 -0.2558283   0.3017889
## 2002   3.3868805   3.7200846   0.16329797   3.17342622  0.2748056  10.1671798
## 2003  -1.8994282  -0.9878865  -5.94840869   3.99131946  0.5119358   1.4843829
## 2004  -0.4386471  -2.9003550   1.14380012  -0.64099565 -1.2416699   5.9246401
## 2005  -4.0692782 -13.8914755   0.09257012  -1.01699490  3.0248903  12.5212797
## 2006   1.3508449   2.6358211   0.10715142   0.04729964 -0.2894381 -12.6922983
## 2007   1.7391272  -1.1568271  -0.47011710   4.96863701  1.4617728   0.9401307
## 2008   3.1639697   1.8887027   5.11960520  -1.10792700  1.3787021   2.6453143
## 2009   2.7955844   5.2327818 -11.23860745   2.46069932  1.5827381  -1.1426075
## 2010   0.9548831   1.2728407   2.39731545  -6.91189540  0.4103455  -9.1348055
## 2011  -2.8668582  -9.2538635  -1.12805468  -2.02654500  3.7307358   1.7752254
## 2012  -2.1615826   4.3282893   2.08242672  -0.31258942 -5.8830510  -3.3541590
## 2013  -6.7585549   1.8432708   2.26863503  -0.39629928 -2.9168291   0.7092182
## 2014 -11.7479432  11.0310816  -3.72879683  -8.85201412 -7.9378003  -8.1255893
## 2015 -15.8285250   1.5682665  -5.34896552  -6.75804086 -8.7787923 -13.1770728
## 2016  -8.9845593  -6.9688493   1.43973387  -7.60298008 -5.1766789 -24.4935331
## 2017 -14.9863614  -3.6408701   1.40894744  -3.58769597 -6.9289439 -15.4903265
## 2018 -13.5788328 -10.1648103   2.92951948 -11.07381110 -4.5559510  -6.9201632
## 2019 -13.1826499   9.3520661   3.44603107  -8.14339630 -5.8061284 -12.2986461
##              50          51          53         54
## 2000  9.3334980  3.52262819 -2.51466703   7.082314
## 2001 -7.5470297  3.76949417  0.68184390   7.907938
## 2002 -6.5770902  0.16162444 -1.02277580   1.651880
## 2003 -1.9011750 -3.77901751  4.39397664   4.535699
## 2004 -5.1006728  5.14870161  2.07462053 -13.245792
## 2005 -5.2770927  3.03572786 -0.67048990   3.057515
## 2006  0.6724051 -0.96986954 -4.55335693  -2.637055
## 2007 -0.2832529 -0.59788780  2.21108611   5.607797
## 2008  5.8980325  4.45490136  4.17312190  -4.072115
## 2009  0.3522267  3.17070597 -2.44856779  -6.761062
## 2010  4.8594316  0.30006093  1.47493946  -7.671514
## 2011  6.8838652 -1.25602873 -2.04069510   2.261032
## 2012 -7.2772523  0.31565145 -1.85751154   3.007931
## 2013  5.3846361 -2.42812390  0.02122725  -2.535492
## 2014  2.0799530 -4.90912715 -3.90396235 -10.993697
## 2015 11.1466987 -8.14496292 -9.25678926  -9.288240
## 2016 25.5202676 -1.53260149 -4.13516149   1.235381
## 2017 12.7525180 -0.24683486 -3.50708670  -1.331951
## 2018 15.5950791 -0.01963506 -3.36883864  14.623048
## 2019 25.5960077 -3.19034963 -3.27040641  15.044138</code></pre>
</div>
<div id="wrangle-state-year-effect-estimates-in-easier-to-read-form" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Wrangle state-year effect estimates in easier-to-read form</h3>
<p>Do some data wrangling to the effect estimates to summarize them.</p>
<p>Notes on my naming conventions for the object:</p>
<ul>
<li>tib: for tibble</li>
<li>diff_eff_by_state_year: difference effects by state-year</li>
<li>mc_nnm: to indicate which gsynth model it corresponds to</li>
</ul>
<p>Also note that these results are among the total population (“overall”), not a particular demographic subgroup.</p>
<pre class="r"><code>tib_diff_eff_by_state_year_mc_nnm=gsynth_out_overall_sub_pol_mc_nnm$eff %&gt;% 
  as_tibble() %&gt;% #Convert to tibble
  mutate(year=row_number()+1999)  %&gt;% #Add 1999 to row number to calculate year
  dplyr::select(year,everything()) %&gt;% 
  pivot_longer(cols=-year) %&gt;%  #make the dataset long-form
  #Rename some of the columns
  rename(
    state_id = name,
    diff_pt = value #estimated difference effect
    ) %&gt;% 
  mutate(state_id=as.numeric(state_id)) #make sure state_id is numeric

tib_diff_eff_by_state_year_mc_nnm</code></pre>
<pre><code>## # A tibble: 680 × 3
##     year state_id diff_pt
##    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
##  1  2000        2  -2.85 
##  2  2000        4   0.925
##  3  2000        5  -6.56 
##  4  2000        6  -6.39 
##  5  2000        8  -2.96 
##  6  2000        9   5.02 
##  7  2000       10  -1.82 
##  8  2000       15   8.08 
##  9  2000       17   1.93 
## 10  2000       18   2.32 
## # ℹ 670 more rows</code></pre>
<p>Before we summarize these difference effect estimates, we can link in the treatment indicator so that we know whether the difference effects are during the post-treatment period. The treatment indicator is obtained by calling <code>gsynth_object$D.tr.</code></p>
<p>(The letter D is conventionally used in the econometrics literature to denote the treatment status.)</p>
<pre class="r"><code>tib_treatedpost =gsynth_out_overall_sub_pol_mc_nnm$D.tr %&gt;% 
  as_tibble() %&gt;%
  mutate(year=row_number()+1999) %&gt;% 
  dplyr::select(year,everything()) %&gt;% 
  pivot_longer(cols=-year) %&gt;% 
  rename(
    state_id = name,
    treatedpost = value) %&gt;% 
  mutate(state_id=as.numeric(state_id))

tib_treatedpost</code></pre>
<pre><code>## # A tibble: 680 × 3
##     year state_id treatedpost
##    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
##  1  2000        2           0
##  2  2000        4           0
##  3  2000        5           0
##  4  2000        6           0
##  5  2000        8           0
##  6  2000        9           0
##  7  2000       10           0
##  8  2000       15           0
##  9  2000       17           0
## 10  2000       18           0
## # ℹ 670 more rows</code></pre>
</div>
<div id="calculate-unweighted-average-difference-effect" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Calculate unweighted average difference effect</h3>
<p>Now we can link those two together to summarize the difference effects over the post-treatment period (which varies by state). It should match the average treatment effect obtained above.</p>
<pre class="r"><code>diff_pt_mean_mc_nnm=tib_diff_eff_by_state_year_mc_nnm %&gt;% 
  #Link the treatment indicator to the difference effects by state-year
  left_join(tib_treatedpost,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;%
  group_by(treatedpost) %&gt;% 
  #take the simple mean by treatment indicator
  summarise(
    diff_pt_mean=mean(diff_pt,na.rm=T)
  )

diff_pt_mean_mc_nnm</code></pre>
<pre><code>## # A tibble: 2 × 2
##   treatedpost diff_pt_mean
##         &lt;dbl&gt;        &lt;dbl&gt;
## 1           0       0.0116
## 2           1      -2.12</code></pre>
<p>The average difference effect post-treatment indeed matches the value returned by <code>gsynth_object$att.avg</code>, which demonstrates that the average treatment effect returned by the gsynth’s default output is the unweighted average over treated state-years.</p>
<pre class="r"><code>gsynth_out_overall_sub_pol_mc_nnm$att.avg</code></pre>
<pre><code>## [1] -2.118079</code></pre>
</div>
<div id="calculate-95-confidence-intervals-for-the-unweighted-average-difference-effect-using-the-bootstrap-replicates." class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Calculate 95% confidence intervals for the unweighted average difference effect using the bootstrap replicates.</h3>
<p>First, return bootstrapped effect estimates for every state-year in every replicate by calling <code>gsynth_object$eff.boot</code>.</p>
<pre class="r"><code>tib_eff_boot_mc_nnm=gsynth_out_overall_sub_pol_mc_nnm$eff.boot %&gt;% 
  #converting to tibble makes the data such that there
  #are 20 rows and 6,800 columns, each with information
  #containing the bootstrapped effect estimates.
  #We need to organize the data such that each row
  #represents a unit-month-bootstrap replicate
  as_tibble() %&gt;% 
  #Note each row represents the time variable.
  #The observation period begins in 2000, so 
  #we should add 1999 to the row number to get the first obs
  #to be 2000
  mutate(year=row_number()+1999) %&gt;% 
  #make year the left-most variable to make it easier to see
  dplyr::select(year,everything()) %&gt;% 
  #now make the data long form
  pivot_longer(cols=-year) %&gt;% 
  #we now have three variables: year, name, and value.
  #the &quot;name&quot; variable contains the state id and the bootstrap
  #replicate. We need to separate these two values into separate variables.
  #We can use separate_wider_delim() for this from tidyr
  separate_wider_delim(name,names=c(&quot;state_id&quot;,&quot;boot_rep&quot;), &quot;.&quot;) %&gt;% 
  #make the state_id and boot_rep (boot replication) variables numeric
  mutate(
    state_id=as.numeric(state_id),
    boot_rep_char=str_sub(boot_rep, 5,11),#5 to 11 in case lots of digits
    boot_rep=as.numeric(boot_rep_char)
  ) %&gt;% 
  dplyr::select(-boot_rep_char) %&gt;%   #drop vars not needed
  #note the difference-based effect estimate is stored in &quot;value&quot;
  #let&#39;s call it diff_boot
  rename(diff_boot =value)

tib_eff_boot_mc_nnm</code></pre>
<pre><code>## # A tibble: 1,360,000 × 4
##     year state_id boot_rep diff_boot
##    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1  2000        2        1    -1.07 
##  2  2000        4        1     9.70 
##  3  2000        5        1     3.89 
##  4  2000        6        1     0.654
##  5  2000        8        1     3.89 
##  6  2000        9        1    -6.57 
##  7  2000       10        1    -4.80 
##  8  2000       15        1    -2.65 
##  9  2000       17        1     3.12 
## 10  2000       18        1    -3.37 
## # ℹ 1,359,990 more rows</code></pre>
<p>Perform some checks on this dataset. The number of rows should equal the number of bootstrap replicates times the number of years times the number of states.</p>
<p>We specified 2,000 boostrap replicates in the <code>gsynth()</code> function above.</p>
<p>There are 34 states included in the analysis.</p>
<pre class="r"><code>n_distinct(tib_eff_boot_mc_nnm$state_id)</code></pre>
<pre><code>## [1] 34</code></pre>
<p>And there are 20 years included in the analysis.</p>
<pre class="r"><code>n_distinct(tib_eff_boot_mc_nnm$year)</code></pre>
<pre><code>## [1] 20</code></pre>
<p>So the number of rows should equal</p>
<pre class="r"><code>2000*n_distinct(tib_eff_boot_mc_nnm$state_id)*n_distinct(tib_eff_boot_mc_nnm$year)</code></pre>
<pre><code>## [1] 1360000</code></pre>
<p>Does it?</p>
<pre class="r"><code>nrow(tib_eff_boot_mc_nnm)</code></pre>
<pre><code>## [1] 1360000</code></pre>
<p>Yes.</p>
<p>Now we can summarize these bootstrapped difference effects to calculate 95% confidence intervals. The process for this is to first calculate average treatment effects for each of the 2,000 bootstrap replicates and then find the 2.5th and 97.5th percentiles over those replicates.</p>
<pre class="r"><code>diff_boot_mean_mc_nnm=tib_eff_boot_mc_nnm %&gt;% 
  #First calculate average treatment effects for each replicate
  left_join(tib_treatedpost,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;%
  group_by(boot_rep, treatedpost) %&gt;% 
  summarise(
    diff_boot_mean=mean(diff_boot,na.rm=T)
  ) %&gt;% 
  ungroup() %&gt;% 
  #We can filter to treatedpost==1 - no need for the pre-treatment effects
  filter(treatedpost==1) %&gt;% 
  #Now find the 2.5th and 97.5th value
  group_by(treatedpost) %&gt;% 
  summarise(
    diff_pt_ll=quantile(diff_boot_mean, probs=0.025, na.rm=TRUE),
    diff_pt_ul=quantile(diff_boot_mean,probs=.975,na.rm=TRUE)
  )</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;boot_rep&#39;. You can override using the
## `.groups` argument.</code></pre>
<pre class="r"><code>diff_boot_mean_mc_nnm</code></pre>
<pre><code>## # A tibble: 1 × 3
##   treatedpost diff_pt_ll diff_pt_ul
##         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1           1      -6.42       1.85</code></pre>
<p>Those are the confidence intervals corresponding to the overall estimate in Scenario 4 in the table in the main text.</p>
</div>
</div>
<div id="calculate-unweighted-average-ratio-effects" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Calculate unweighted average ratio effects</h2>
<div id="point-estimates" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Point estimates</h3>
<p>Ratio effects can also be calculated using this method by comparing the predicted counterfactual outcomes in the treated (<code>gsynth_object$Y.ct</code>) with the actual outcomes (<code>gsynth_object$Y.tr</code>).</p>
<pre class="r"><code>#actual outcome in treated
tib_y_tr_mc_nnm=gsynth_out_overall_sub_pol_mc_nnm$Y.tr %&gt;% 
  as_tibble() %&gt;%
  mutate(year=row_number()+1999) %&gt;% 
  dplyr::select(year,everything()) %&gt;% 
  pivot_longer(cols=-year) %&gt;% 
  rename(
    state_id = name,
    y_tr_pt = value #observed treated value, pt for point estimate
  ) %&gt;% 
  mutate(state_id =as.numeric(state_id))
  
  
# Counterfactual outcome in treated
tib_y_ct_mc_nnm=gsynth_out_overall_sub_pol_mc_nnm$Y.ct %&gt;% 
  as_tibble() %&gt;%
  mutate(year=row_number()+1999) %&gt;% 
  dplyr::select(year,everything()) %&gt;% 
  pivot_longer(cols=-year) %&gt;% 
  rename(
    state_id = name,
    y_ct_pt = value#estimated counterfactual value, pt for point estimate
  ) %&gt;% 
  mutate(state_id =as.numeric(state_id))</code></pre>
<p>Link them together with the treatment-status indicator and find differences and ratios</p>
<pre class="r"><code>tib_mc_nnm=tib_treatedpost %&gt;% 
  left_join(tib_y_tr_mc_nnm,by=c(&quot;year&quot;,&quot;state_id&quot;)) %&gt;% 
  left_join(tib_y_ct_mc_nnm,by=c(&quot;year&quot;,&quot;state_id&quot;)) %&gt;% 
  left_join(tib_diff_eff_by_state_year_mc_nnm,by=c(&quot;year&quot;,&quot;state_id&quot;)) %&gt;% 
  #Check to make sure that the difference calculated
  #by subtracting the observed outcome from the counterfactual outcome
  #is the same as the reported state-year effect estimates. 
  #Rename the difference effect reported by .eff
  rename(diff_pt_report=diff_pt) %&gt;% 
  #Calculate differences and ratios
  mutate(
      diff_pt=y_tr_pt-y_ct_pt,#difference effect (point estimate)
      ratio_pt=y_tr_pt/y_ct_pt, #Ratio effect (point estimate)
      
#      As a check, are these the same?
      diff_pt_check=diff_pt_report-diff_pt

  )
  

tib_mc_nnm</code></pre>
<pre><code>## # A tibble: 680 × 9
##     year state_id treatedpost y_tr_pt y_ct_pt diff_pt_report diff_pt ratio_pt
##    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1  2000        2           0    155.    158.         -2.85   -2.85     0.982
##  2  2000        4           0    173.    172.          0.925   0.925    1.01 
##  3  2000        5           0    252.    259.         -6.56   -6.56     0.975
##  4  2000        6           0    168.    174.         -6.39   -6.39     0.963
##  5  2000        8           0    130     133.         -2.96   -2.96     0.978
##  6  2000        9           0    158     153.          5.02    5.02     1.03 
##  7  2000       10           0    195.    197.         -1.82   -1.82     0.991
##  8  2000       15           0    185.    177.          8.08    8.08     1.05 
##  9  2000       17           0    208.    206.          1.93    1.93     1.01 
## 10  2000       18           0    214.    212.          2.32    2.32     1.01 
## # ℹ 670 more rows
## # ℹ 1 more variable: diff_pt_check &lt;dbl&gt;</code></pre>
<p>As a check on these calculations, <code>diff_pt_check</code> should be zero in all observations</p>
<pre class="r"><code>summary(tib_mc_nnm$diff_pt_check)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       0       0       0       0       0       0</code></pre>
<p>We can now calculate average ratio effects in the treated over treated state-years. The ratio effect estimand of interest is the mean of the observed outcome in the treated divided by the mean of the countefactual outcome in the treated. In general, the arithmetic mean of ratios is not generally the same as the ratio of arithmetic means, so we should not take the arithmetic mean of the state-year ratio effects, We can find the summary ratio effect by finding the mean of treated counterfactual outcomes in the treated state-years and the mean of observed outcomes in the treated state-years and then taking the ratio of those two means.</p>
<pre class="r"><code>ratio_pt_mc_nnm=tib_mc_nnm %&gt;% 
  filter(treatedpost==1) %&gt;% 
  group_by(treatedpost) %&gt;% 
  summarise(
    y_tr_pt_mean=mean(y_tr_pt,na.rm=T),#observed treated value
    y_ct_pt_mean=mean(y_ct_pt,na.rm=T),#counterfactual 
  ) %&gt;% 
  ungroup() %&gt;% 
  mutate(
    ratio_pt_mean=y_tr_pt_mean/y_ct_pt_mean #ratio effect, point estimate
  )

ratio_pt_mc_nnm</code></pre>
<pre><code>## # A tibble: 1 × 4
##   treatedpost y_tr_pt_mean y_ct_pt_mean ratio_pt_mean
##         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;
## 1           1         144.         146.         0.986</code></pre>
<p>This estimated ratio effect of 0.99 corresponds to the reported overall ratio effect for scenario 4 in the table in the main text.</p>
</div>
<div id="confidence-intervals" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Confidence intervals</h3>
<p>As we did for difference effects, we can calculate confidence intervals around this ratio effect by calculating the ratio effect in each bootstrap replicate and then finding the 2.5th and 97.5th percentiles over replicates.</p>
<p>In each replicate, we can calculate the estimated counterfactual outcome by subtracting the bootstrap’s difference effect from the observed outcome.</p>
<pre class="r"><code>#We can work from the previous tibble we created corresponding to the difference-based 
#bootstrapped effect estimates
ratio_ci_mc_nnm=tib_eff_boot_mc_nnm %&gt;% 
  left_join(tib_mc_nnm,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;% #link in the data just above
  filter(treatedpost==1) %&gt;% #limit to treated observations
  mutate(
    y_ct_boot=y_tr_pt-diff_boot #counterfactual estimate - bootstrap
  ) %&gt;% 
  #now calculate summary ratio effect in each replicate
  group_by(boot_rep, treatedpost) %&gt;% 
  summarise(
      y_tr_pt_mean=mean(y_tr_pt,na.rm=T),#observed treated value, mean (doesn&#39;t vary by boot rep)
      y_ct_boot_mean=mean(y_ct_boot,na.rm=T)
  ) %&gt;% 
  ungroup() %&gt;% 
  mutate(
    ratio_boot_mean=y_tr_pt_mean/y_ct_boot_mean
  ) %&gt;% 
    #Now return the 95% confidence intervals of the mean bootstrapped 
  #difference effect over replicates
  group_by(treatedpost) %&gt;% 
  summarise(
      ratio_pt_ll=quantile(ratio_boot_mean, probs=0.025, na.rm=TRUE),
      ratio_pt_ul=quantile(ratio_boot_mean,probs=.975,na.rm=TRUE)
  )</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;boot_rep&#39;. You can override using the
## `.groups` argument.</code></pre>
<p>Ratio effect point estimate</p>
<pre class="r"><code>ratio_pt_mc_nnm</code></pre>
<pre><code>## # A tibble: 1 × 4
##   treatedpost y_tr_pt_mean y_ct_pt_mean ratio_pt_mean
##         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;
## 1           1         144.         146.         0.986</code></pre>
<p>Ratio effect 95% CI</p>
<pre class="r"><code>ratio_ci_mc_nnm</code></pre>
<pre><code>## # A tibble: 1 × 3
##   treatedpost ratio_pt_ll ratio_pt_ul
##         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1           1       0.957        1.01</code></pre>
</div>
</div>
<div id="assess-model-fit-before-treatment" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Assess model fit before treatment</h2>
<p>We can also assess model fit before treatment using the difference-based “effect” estimates before treatment. Before treatment, these differences are not effects but are the pre-treatment prediction error.</p>
<p>In the main text of the table, I used mean absolute error to measure model fit. Mean squared error or root mean squared error could also be used. I calculate these three measures below.</p>
<pre class="r"><code>#Let&#39;s begin with this tibble, which already contains all of the difference-based
#effect estimates
pre_tx_fit_mc_nnm=tib_mc_nnm %&gt;% 
  mutate(
    #The mean absolute error is the mean of the absolute value of the difference-based
    #effect estimates before treatment.
    diff_pt_abs=abs(diff_pt),

    #To calculate mean squared error, we can first square the difference-based effect
    #estimates and then take the mean
    diff_pt_squared=diff_pt**2 
  ) %&gt;% 
  filter(treatedpost==0) %&gt;% #0 meaning pre-treatment
  group_by(treatedpost) %&gt;% 
  summarise(
    diff_pt_abs_mean=mean(diff_pt_abs,na.rm=T),
    diff_pt_squared_mean=mean(diff_pt_squared,na.rm=T)
  ) %&gt;% 
  mutate(
      #square root the square error
      diff_pt_root_mean_square=sqrt(diff_pt_squared_mean)
  )</code></pre>
<p>The mean absolute error of 3.3 corresponds to the reported value for scenario 4 in the table in the main text</p>
<pre class="r"><code>pre_tx_fit_mc_nnm</code></pre>
<pre><code>## # A tibble: 1 × 4
##   treatedpost diff_pt_abs_mean diff_pt_squared_mean diff_pt_root_mean_square
##         &lt;dbl&gt;            &lt;dbl&gt;                &lt;dbl&gt;                    &lt;dbl&gt;
## 1           0             3.32                 19.0                     4.36</code></pre>
</div>
<div id="weighted-average-treatment-effects" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Weighted average treatment effects</h2>
<p>When considering the overall population-level effect of the policy, it may be desirable to weight states proportional to their share of that total population (or, more precisely, person-time as there is a temporal component). In this section, I calculate a weighted-average treatment effect, weighting each treated state-year’s effect estimate by its share of treated person time.</p>
<div id="exploring-weights" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Exploring weights</h3>
<p>The weights for a given state-year are that state-year’s proportion of the total treated person-years among adults aged 45-64 (variable name: <code>prop_of_tot_pop_year</code>). I gathered state-year populations of adults aged 45-64 from the American Community Survey. These scripts have more details on that:</p>
<ul>
<li>scripts/read-wrangle-data-acs.R</li>
<li>scripts/read-wrangle-data-nianogo-et-al.R</li>
</ul>
<p>Here a histogram of the weights in the treated state-years.</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;state_year_wts_overall.RData&quot;)
state_year_wts_overall %&gt;% 
  ggplot(aes(prop_of_tot_pop_year))+
  geom_histogram()+
  theme_bw()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>Each observation in this dataset of weights is a state-year. The weights add up to 1 over treated state-years.</p>
<pre class="r"><code>state_year_wts_overall</code></pre>
<pre><code>## # A tibble: 187 × 4
##    state_id  year pop_tot_all_treated_years prop_of_tot_pop_year
##       &lt;dbl&gt; &lt;dbl&gt;                     &lt;dbl&gt;                &lt;dbl&gt;
##  1        4  2014                 309495338             0.00527 
##  2        5  2014                 309495338             0.00242 
##  3        6  2014                 309495338             0.0315  
##  4        8  2014                 309495338             0.00446 
##  5        9  2014                 309495338             0.00335 
##  6       10  2014                 309495338             0.000817
##  7       15  2014                 309495338             0.00116 
##  8       17  2014                 309495338             0.0110  
##  9       19  2014                 309495338             0.00264 
## 10       20  2014                 309495338             0.00235 
## # ℹ 177 more rows</code></pre>
<pre class="r"><code>state_year_wts_overall %&gt;% 
  mutate(dummy=1) %&gt;% 
  group_by(dummy) %&gt;% 
  summarise(prop_of_tot_pop_year=sum(prop_of_tot_pop_year))</code></pre>
<pre><code>## # A tibble: 1 × 2
##   dummy prop_of_tot_pop_year
##   &lt;dbl&gt;                &lt;dbl&gt;
## 1     1                 1.00</code></pre>
<p>Here’s a map of the weights in 2019. In a given year, they do not sum to 1 because they sum to 1 over all treated state-years.</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;lookup_state_id_state_abb.RData&quot;)
state_year_wts_overall_2019=state_year_wts_overall %&gt;% 
  filter(year==2019)
lookup_state_abb_geo_simplify_shift_al_hi %&gt;% 
  left_join(lookup_state_id_state_abb,by=&quot;state_abb&quot;) %&gt;% 
  left_join(state_year_wts_overall_2019,by=&quot;state_id&quot;) %&gt;% 
  tm_shape()+
  tm_fill(
        &quot;prop_of_tot_pop_year&quot;,
        palette =viridis(n=5),
  title=&quot;Share of person-years&quot;)+
  tm_borders(col=&quot;black&quot;,alpha=1)+
    tm_layout(
    frame=F,
#    legend.title.size=.5,
    legend.outside=T,
    legend.outside.position=&quot;bottom&quot;
    )</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>The same map in 2014. There are fewer states with non-missing data in this map because fewer states had expanded Medicaid in 2014.</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;lookup_state_id_state_abb.RData&quot;)
state_year_wts_overall_2014=state_year_wts_overall %&gt;% 
  filter(year==2014)
lookup_state_abb_geo_simplify_shift_al_hi %&gt;% 
  left_join(lookup_state_id_state_abb,by=&quot;state_abb&quot;) %&gt;% 
  left_join(state_year_wts_overall_2014,by=&quot;state_id&quot;) %&gt;% 
  tm_shape()+
  tm_fill(
        &quot;prop_of_tot_pop_year&quot;,
        palette =viridis(n=5),
  title=&quot;Share of person-years&quot;)+
  tm_borders(col=&quot;black&quot;,alpha=1)+
    tm_layout(
    frame=F,
#    legend.title.size=.5,
    legend.outside=T,
    legend.outside.position=&quot;bottom&quot;
    )</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
</div>
<div id="weighting-effect-estimates" class="section level3" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Weighting effect estimates</h3>
<p>We can use these weights to weight each treated year’s effect estimate to calculate a weighted average treatment effect.</p>
<p>Work from the <code>tib_mc_nnm</code> tibble created above, which contains the following for each state-year estimated by the MC-NNM estimator:
* counterfactual outcome
* observed outcome
* estimated difference effect
* estimated ratio effect
* treatment status indicator</p>
<p>To this dataset, we will add the weights (<code>prop_of_tot_pop_year</code>, described above), joining by year and state identifier. To calculate the weighted-mean difference effect, we can take the weighted average of the constituent difference effects over treatment status (<code>treatedpost</code>) using base R’s <code>weighted.mean()</code> function.</p>
<p>To calculate the weighted ratio effect, we first calculate weighted-average counterfactual and observed outcomes and then take the ratio of those.</p>
<pre class="r"><code>tib_mc_nnm %&gt;% 
  left_join(state_year_wts_overall,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;% 
  filter(treatedpost==1) %&gt;% 
  group_by(treatedpost) %&gt;% 
  summarise(
    
    #weighted mean difference effect
    diff_pt_mean_wt=weighted.mean(
      x=diff_pt,
      w=prop_of_tot_pop_year,#proportion of total population-years
      na.rm=T),
    
    #unweighted mean difference effect for comparison
    diff_pt_mean_unwt=mean(diff_pt,na.rm=T), 

    #weighted average observed outcome
      y_tr_pt_mean_wt=weighted.mean(
        x=y_tr_pt,
        w=prop_of_tot_pop_year, 
        na.rm=T),
    
    #weighted average counterfactual outcome
    y_ct_pt_mean_wt=weighted.mean(
        x=y_ct_pt,
        w=prop_of_tot_pop_year, 
        na.rm=T)
    ) %&gt;% 
  ungroup() %&gt;% 
  mutate(
    #Calculate ratio of the weighted observed outcome to weighted counterfactual outcome
    ratio_pt_mean_wt=y_tr_pt_mean_wt/y_ct_pt_mean_wt
  ) %&gt;% 
  dplyr::select(-starts_with(&quot;treated&quot;)) %&gt;% #remove this column for space
  knitr::kable(
    caption=&quot;Weighted average treatment effects in treated&quot;,
    digits=2)</code></pre>
<table style="width:100%;">
<caption><span id="tab:unnamed-chunk-59">Table 3.1: </span>Weighted average treatment effects in treated</caption>
<colgroup>
<col width="19%" />
<col width="21%" />
<col width="19%" />
<col width="19%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">diff_pt_mean_wt</th>
<th align="right">diff_pt_mean_unwt</th>
<th align="right">y_tr_pt_mean_wt</th>
<th align="right">y_ct_pt_mean_wt</th>
<th align="right">ratio_pt_mean_wt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-5.22</td>
<td align="right">-2.12</td>
<td align="right">140.14</td>
<td align="right">145.36</td>
<td align="right">0.96</td>
</tr>
</tbody>
</table>
<p>Confidence intervals can be calculated analogously, first calculating the above measures in every bootstrap replicate and then taking the percentiles over replicates.</p>
<pre class="r"><code>tib_eff_boot_mc_nnm %&gt;% 
  left_join(tib_mc_nnm,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;% #link in the data just above
  left_join(state_year_wts_overall,by=c(&quot;state_id&quot;,&quot;year&quot;)) %&gt;% 
  filter(treatedpost==1) %&gt;% #limit to treated observations
  mutate(
    y_ct_boot=y_tr_pt-diff_boot #counterfactual estimate - bootstrap
  ) %&gt;% 
  #calculate weighted averages in each replicate
  group_by(boot_rep, treatedpost) %&gt;%
  summarise(
    
      #mean of the counterfactual estimate in the bootstrap rep
      #Unweighted
      y_ct_boot_mean_wt=weighted.mean(
        x=y_ct_boot,
        w=prop_of_tot_pop_year,
        na.rm=T),
      
      #calculate this again (doesn&#39;t change between reps)
      y_tr_pt_mean_wt=weighted.mean(
        x=y_tr_pt,
        w=prop_of_tot_pop_year,
        na.rm=T)
  ) %&gt;% 
  ungroup() %&gt;% 
  mutate(
    #weighted differences and ratios in each replicate
    ratio_boot_mean_wt=y_tr_pt_mean_wt/y_ct_boot_mean_wt,
    diff_boot_mean_wt=y_tr_pt_mean_wt-y_ct_boot_mean_wt
  ) %&gt;% 
  #Find percentiles over replicates
  group_by(treatedpost) %&gt;% 
  summarise(
      diff_pt_wt_ll=quantile(diff_boot_mean_wt, probs=0.025, na.rm=TRUE),
      diff_pt_wt_ul=quantile(diff_boot_mean_wt,probs=.975,na.rm=TRUE),
      
      ratio_pt_wt_ll=quantile(ratio_boot_mean_wt, probs=0.025, na.rm=TRUE),
      ratio_pt_wt_ul=quantile(ratio_boot_mean_wt,probs=.975,na.rm=TRUE)

  ) %&gt;% 
  ungroup() %&gt;% 
  knitr::kable(
    caption=&quot;95% CIs for weighted average treatment effects in treated&quot;,
    digits=2)</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;boot_rep&#39;. You can override using the
## `.groups` argument.</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-60">Table 3.2: </span>95% CIs for weighted average treatment effects in treated</caption>
<colgroup>
<col width="17%" />
<col width="20%" />
<col width="20%" />
<col width="21%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">treatedpost</th>
<th align="right">diff_pt_wt_ll</th>
<th align="right">diff_pt_wt_ul</th>
<th align="right">ratio_pt_wt_ll</th>
<th align="right">ratio_pt_wt_ul</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">-7.65</td>
<td align="right">2.81</td>
<td align="right">0.95</td>
<td align="right">1.02</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="support-for-in-text-statements" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Support for in-text statements</h1>
<p>This section includes supporting information for some statements that I made in the text.</p>
<div id="negative-predicted-counterfactual-outomes-in-some-groups" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Negative predicted counterfactual outomes in some groups</h2>
<p>I stated the following in the main text: “the minimum predicted value of the counterfactual outcome in the treatment period over the replicates for the Hispanic population in Scenario 2 was −6091 CVD deaths per 100,000 adults.”</p>
<p>The gsynth model output for the Hispanic population under Scenario 2 is created here:
gsynth-nianogo-et-al/scripts/gsynth-analyses-to-post.R</p>
<p>I’m loading its cleaned-up output here</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;gsynth_out_tib_hispanic_sub_pol.RData&quot;)</code></pre>
<p>Below is the distribution of the predicted counterfactual outcomes. The minimum value is below zero, which is not plausible for a mortality rate.</p>
<pre class="r"><code>summary(gsynth_out_tib_hispanic_sub_pol$y_ct_boot)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1995.6    80.3   106.0   116.2   133.9  3110.6</code></pre>
<pre class="r"><code>gsynth_out_tib_hispanic_sub_pol %&gt;% 
  ggplot(aes(x=y_ct_boot))+
  geom_histogram()+
  theme_bw()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
</div>
<div id="implausible-demographic-values" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Implausible demographic values</h2>
<p>In the main text, I also noted that there were implausible values in age-group-race subgroups. I stated that, “in one state-year, the data state that 100% of Hispanic adults aged 45 to 64 years were men, while in another state-year, 0% were.”</p>
<p>The histograms in the sub-section titled “Exploring data”&gt;“Distribution of BRFSS covariates”&gt;“Proportion men” shows that data. Copied here:</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.3693  0.4200  0.4188  0.4651  1.0000</code></pre>
<p><img src="comment-appendix_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
</div>
<div id="state-by-state-share-of-total-person-years-of-adults-aged-45-64" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> State-by-state share of total person-years of adults aged 45-64</h2>
<p>In the text, I stated, “Considering the total population of adults aged 45 to 64 years, for example, there were 309,495,338 person-years over all treated states and years. California contributed the largest share (19.1%) to this total. Considering Black adults in this age range, the state of New York contributed the largest share (15.8%) of the 29,839,638 total treated person-years available for analysis.”</p>
<p>Total person-years of adults aged 45-64 over all treated states and years</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;state_year_wts_overall.RData&quot;)
state_year_wts_overall %&gt;% slice(1) %&gt;% pull(pop_tot_all_treated_years)</code></pre>
<pre><code>## [1] 309495338</code></pre>
<p>Share of that total contributed by each state</p>
<pre class="r"><code>state_year_wts_overall  %&gt;%
  left_join(lookup_state_id_state_abb,by=&quot;state_id&quot;) %&gt;%
  dplyr::select(state_abb,everything()) %&gt;%
  group_by(state_abb) %&gt;% 
  summarise(prop_of_tot_pop_year=sum(prop_of_tot_pop_year,na.rm=T)) %&gt;% 
  ungroup() %&gt;% 
  arrange(desc(prop_of_tot_pop_year))</code></pre>
<pre><code>## # A tibble: 34 × 2
##    state_abb prop_of_tot_pop_year
##    &lt;chr&gt;                    &lt;dbl&gt;
##  1 CA                      0.191 
##  2 NY                      0.101 
##  3 IL                      0.0648
##  4 OH                      0.0607
##  5 PA                      0.0566
##  6 MI                      0.0528
##  7 NJ                      0.0480
##  8 WA                      0.0365
##  9 MA                      0.0362
## 10 AZ                      0.0327
## # ℹ 24 more rows</code></pre>
<p>Total person-years of Black adults aged 45-64 over all treated states and years</p>
<pre class="r"><code>setwd(here(&quot;data&quot;,&quot;data-processed&quot;))
load(&quot;state_year_wts_race_b.RData&quot;)
state_year_wts_race_b %&gt;% slice(1) %&gt;% pull(pop_tot_all_treated_years)</code></pre>
<pre><code>## [1] 29839638</code></pre>
<p>Share of that total contributed by each treated state</p>
<pre class="r"><code>state_year_wts_race_b %&gt;%
  left_join(lookup_state_id_state_abb,by=&quot;state_id&quot;) %&gt;%
  dplyr::select(state_abb,everything()) %&gt;%
  group_by(state_abb) %&gt;% 
  summarise(prop_of_tot_pop_year=sum(prop_of_tot_pop_year,na.rm=T)) %&gt;% 
  ungroup() %&gt;% 
  arrange(desc(prop_of_tot_pop_year))</code></pre>
<pre><code>## # A tibble: 34 × 2
##    state_abb prop_of_tot_pop_year
##    &lt;chr&gt;                    &lt;dbl&gt;
##  1 NY                      0.158 
##  2 CA                      0.120 
##  3 MD                      0.0959
##  4 IL                      0.0907
##  5 OH                      0.0704
##  6 MI                      0.0672
##  7 NJ                      0.0631
##  8 PA                      0.0573
##  9 LA                      0.0483
## 10 MA                      0.0250
## # ℹ 24 more rows</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
